{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python373jvsc74a57bd0c3b3477b6868e90e11dcbd59a1675846cbaf9059aabcf4243fe25ef4fa2d4910",
   "display_name": "Python 3.7.3 64-bit ('deer': virtualenv)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FOLDER = '../tests/data/check_format/ok' \n",
    "INPUT = os.path.join(INPUT_FOLDER, 'test1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = open(INPUT).read()\n",
    "#input_data = tf.io.read_file(INPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lazy_char_read(input_file):\n",
    "    def lazy_char_read():\n",
    "        with open(input_file) as f:\n",
    "            while True:\n",
    "                data = f.read(1)\n",
    "                if not data:\n",
    "                    break\n",
    "                yield data \n",
    "    return lazy_char_read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LENGTH = 20\n",
    "EXAMPLES_PER_EPOCH = len(input_tokens) // SEQ_LENGTH\n",
    "\n",
    "input_dset = tf.data.Dataset.from_generator(make_lazy_char_read(INPUT), output_types=tf.string)\n",
    "# sequences = input_dset.batch(SEQ_LENGTH+1, drop_remainder=True)\n",
    "sequences = input_dset.window(SEQ_LENGTH+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[b'C', b'A', b'N', b'T', b'O', b' ', b'P', b'R', b'I', b'M', b'O', b'\\n', b'\\n', b' ', b' ', b'N', b'e', b'l', b' ', b'm', b'e']\n",
      "[b'z', b'z', b'o', b' ', b'd', b'e', b'l', b' ', b'c', b'a', b'm', b'm', b'i', b'n', b' ', b'd', b'i', b' ', b'n', b'o', b's']\n",
      "[b't', b'r', b'a', b' ', b'v', b'i', b't', b'a', b'\\n', b'm', b'i', b' ', b'r', b'i', b't', b'r', b'o', b'v', b'a', b'i', b' ']\n",
      "[b'p', b'e', b'r', b' ', b'u', b'n', b'a', b' ', b's', b'e', b'l', b'v', b'a', b' ', b'o', b's', b'c', b'u', b'r', b'a', b'\\n']\n",
      "[b'c', b'h', b'\\xc3\\xa9', b' ', b'l', b'a', b' ', b'd', b'i', b'r', b'i', b't', b't', b'a', b' ', b'v', b'i', b'a', b' ', b'e', b'r']\n",
      "[b'a', b' ', b's', b'm', b'a', b'r', b'r', b'i', b't', b'a', b'.', b'\\n', b' ', b' ', b'A', b'h', b' ', b'q', b'u', b'a', b'n']\n",
      "[b't', b'o', b' ', b'a', b' ', b'd', b'i', b'r', b' ', b'q', b'u', b'a', b'l', b' ', b'e', b'r', b'a', b' ', b'\\xc3\\xa8', b' ', b'c']\n",
      "[b'o', b's', b'a', b' ', b'd', b'u', b'r', b'a', b'\\n', b'e', b's', b't', b'a', b' ', b's', b'e', b'l', b'v', b'a', b' ', b's']\n",
      "[b'e', b'l', b'v', b'a', b'g', b'g', b'i', b'a', b' ', b'e', b' ', b'a', b's', b'p', b'r', b'a', b' ', b'e', b' ', b'f', b'o']\n",
      "[b'r', b't', b'e', b'\\n', b'c', b'h', b'e', b' ', b'n', b'e', b'l', b' ', b'p', b'e', b'n', b's', b'i', b'e', b'r', b' ', b'r']\n",
      "[b'i', b'n', b'o', b'v', b'a', b' ', b'l', b'a', b' ', b'p', b'a', b'u', b'r', b'a', b'!', b'\\n', b' ', b' ', b'T', b'a', b'n']\n",
      "[b't', b\"'\", b'\\xc3\\xa8', b' ', b'a', b'm', b'a', b'r', b'a', b' ', b'c', b'h', b'e', b' ', b'p', b'o', b'c', b'o', b' ', b'\\xc3\\xa8', b' ']\n",
      "[b'p', b'i', b'\\xc3\\xba', b' ', b'm', b'o', b'r', b't', b'e', b';', b'\\n', b'm', b'a', b' ', b'p', b'e', b'r', b' ', b't', b'r', b'a']\n",
      "[b't', b't', b'a', b'r', b' ', b'd', b'e', b'l', b' ', b'b', b'e', b'n', b' ', b'c', b'h', b\"'\", b'i', b'o', b' ', b'v', b'i']\n",
      "[b' ', b't', b'r', b'o', b'v', b'a', b'i', b',', b'\\n']\n"
     ]
    }
   ],
   "source": [
    "for window in sequences:\n",
    "  print(list(window.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = tf.keras.preprocessing.text.Tokenizer(\n",
    "    num_words=None,\n",
    "    filters='',\n",
    "    char_level=True,\n",
    "    lower=False,\n",
    "    oov_token='UNK'\n",
    ")\n",
    "tok.fit_on_texts(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens = tok.texts_to_sequences([input_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[29, 21, 22, 23, 24, 2, 30, 31, 32, 33, 24, 9, 9, 2, 2, 22, 5, 10, 2, 14, 5, 25, 25, 8, 2, 17, 5, 10, 2, 15, 3, 14, 14, 6, 12, 2, 17, 6, 2, 12, 8, 11, 7, 4, 3, 2, 13, 6, 7, 3, 9, 14, 6, 2, 4, 6, 7, 4, 8, 13, 3, 6, 2, 16, 5, 4, 2, 18, 12, 3, 2, 11, 5, 10, 13, 3, 2, 8, 11, 15, 18, 4, 3, 9, 15, 19, 34, 2, 10, 3, 2, 17, 6, 4, 6, 7, 7, 3, 2, 13, 6, 3, 2, 5, 4, 3, 2, 11, 14, 3, 4, 4, 6, 7, 3, 35, 9, 2, 2, 21, 19, 2, 26, 18, 3, 12, 7, 8, 2, 3, 2, 17, 6, 4, 2, 26, 18, 3, 10, 2, 5, 4, 3, 2, 20, 2, 15, 8, 11, 3, 2, 17, 18, 4, 3, 9, 5, 11, 7, 3, 2, 11, 5, 10, 13, 3, 2, 11, 5, 10, 13, 3, 27, 27, 6, 3, 2, 5, 2, 3, 11, 16, 4, 3, 2, 5, 2, 36, 8, 4, 7, 5, 9, 15, 19, 5, 2, 12, 5, 10, 2, 16, 5, 12, 11, 6, 5, 4, 2, 4, 6, 12, 8, 13, 3, 2, 10, 3, 2, 16, 3, 18, 4, 3, 37, 9, 2, 2, 23, 3, 12, 7, 28, 20, 2, 3, 14, 3, 4, 3, 2, 15, 19, 5, 2, 16, 8, 15, 8, 2, 20, 2, 16, 6, 38, 2, 14, 8, 4, 7, 5, 39, 9, 14, 3, 2, 16, 5, 4, 2, 7, 4, 3, 7, 7, 3, 4, 2, 17, 5, 10, 2, 40, 5, 12, 2, 15, 19, 28, 6, 8, 2, 13, 6, 2, 7, 4, 8, 13, 3, 6, 41, 9]]\n"
     ]
    }
   ],
   "source": [
    "print(input_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating text from sequence (mapping back)\n",
    "def char_lvl_decode(tok, sequences):\n",
    "    word_index = tok.word_index\n",
    "    reverse_map = {val:key for key, val in word_index.items()}\n",
    "    \n",
    "    decoded_sequences = []\n",
    "    for seq in sequences:\n",
    "        retext = ''\n",
    "        for q in seq:\n",
    "            retext += reverse_map[q]\n",
    "        #''.join(retext)\n",
    "        decoded_sequences.append(retext) \n",
    "    return decoded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_data = char_lvl_decode(tok, input_tokens)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "for orig, dec in zip(input_data, decoded_data):\n",
    "    assert len(orig) == len(dec) \n",
    "    for o,d in zip(orig, dec):\n",
    "        assert o == d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in sequences.take(1):\n",
    "    print('a')\n",
    "    decoded = t.sequences_to_texts(item)\n",
    "    print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<TakeDataset shapes: (101, 303), types: tf.int32>\n"
     ]
    }
   ],
   "source": [
    "print(sequences.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dset = tf.keras.preprocessing.text_dataset_from_directory(INPUT_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}